{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a1ed38",
   "metadata": {},
   "source": [
    "# NLP Sentiment Analysis\n",
    "\n",
    "Perform sentiment analysis using Python's NLTK (Natural Language Toolkit) library.\n",
    "\n",
    "Use the `movie_reviews` corpus, which contains 2,000 movie reviews pre-labeled as either \"positive\" or \"negative.\"\n",
    "\n",
    "Build a **Naive Bayes classifier** - a common and effective model for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04bb8a6",
   "metadata": {},
   "source": [
    "### Download NLTK Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b887e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading movie_reviews: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1028)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1028)>\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1028)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the nltk module\n",
    "import nltk\n",
    "\n",
    "# Download data\n",
    "nltk.download('movie_reviews') # corpus of reviews\n",
    "nltk.download('stopwords') # corpus of stopwords\n",
    "nltk.download('punkt') # tokenizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca808d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# Load the movie_reviews corpus\n",
    "# Creating a list of (review, sentiment) tuples\n",
    "documents = []\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        # Add a tuple of (list_of_words, category)\n",
    "        documents.append((list(movie_reviews.words(fileid)), category))\n",
    "\n",
    "# Shuffle for better training and testing\n",
    "random.shuffle(documents)\n",
    "\n",
    "# Display results\n",
    "print(f\"Loaded {len(documents)} documents.\")\n",
    "print(\"First document (first 20 words):\", documents[0][0][:20])\n",
    "print(\"Sentiment:\", documents[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8ab763",
   "metadata": {},
   "source": [
    "### Preprocessing and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aaaa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 2: Loading and Preparing the Data\n",
    "\n",
    "We will load the `movie_reviews` corpus. \n",
    "It's structured as a list of file IDs, which we can then categorize into positive and negative reviews. '\n",
    "'We'll create a list of `(review, sentiment)` tuples.\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "### Step 3: Text Preprocessing and Feature Extraction\n",
    "\n",
    "A machine learning model can't understand raw text. We need to convert each review into a numerical format. We will use a **\"Bag-of-Words\"** model.\n",
    "\n",
    "Our \"features\" will be a dictionary for each review, where the keys are the most common words in the *entire* dataset, and the values are `True` or `True` depending on whether that word is in the review.\n",
    "\n",
    "**1. Clean and Tokenize Words**\n",
    "First, let's get a clean list of all words from all reviews. We'll convert them to lowercase and filter out **stopwords** (common words like \"the\", \"is\", \"a\") and punctuation, as they don't carry much sentiment.\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Get all words from all reviews, make them lowercase, and remove stopwords/punctuation\n",
    "all_words = []\n",
    "for w_list, category in documents:\n",
    "    for w in w_list:\n",
    "        if w.lower() not in stop_words and w.lower() not in string.punctuation:\n",
    "            all_words.append(w.lower())\n",
    "\n",
    "# Get the frequency distribution of all words\n",
    "all_words_freq = nltk.FreqDist(all_words)\n",
    "\n",
    "# Print the 20 most common words\n",
    "print(\"Most common words:\", all_words_freq.most_common(20))\n",
    "\n",
    "# We will use the top 3000 most common words as our features\n",
    "word_features = [item[0] for item in all_words_freq.most_common(3000)]\n",
    "```\n",
    "\n",
    "**2. Create the Feature Sets**\n",
    "Now, we create a function that will take a review and create the feature dictionary we described.\n",
    "\n",
    "```python\n",
    "def find_features(document_words):\n",
    "    \"\"\"\n",
    "    Takes a list of words from a review and returns a dictionary\n",
    "    of features indicating which of the top 3000 words are present.\n",
    "    \"\"\"\n",
    "    words_in_doc = set(document_words)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words_in_doc)\n",
    "    return features\n",
    "\n",
    "# Create feature sets for all our documents\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "\n",
    "# Example of one feature set\n",
    "print(\"\\n--- Example Feature Set ---\")\n",
    "print(featuresets[0][0])\n",
    "print(\"Sentiment:\", featuresets[0][1])\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "### Step 4: Training the Naive Bayes Classifier\n",
    "\n",
    "With our data correctly formatted into `featuresets`, we can now train our classifier. We'll split the data into a **training set** (to teach the model) and a **test set** (to evaluate its performance on unseen data).\n",
    "\n",
    "A common split is 80% for training and 20% for testing. Our 2,000 documents will be split into 1,600 for training and 400 for testing.\n",
    "\n",
    "```python\n",
    "# Split the data into training and testing sets\n",
    "training_set = featuresets[:1600]\n",
    "testing_set = featuresets[1600:]\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "print(\"\\nTraining the classifier...\")\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Classifier trained successfully.\")\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "### Step 5: Evaluating the Model\n",
    "\n",
    "Now that the classifier is trained, let's see how well it performs on the `testing_set` it has never seen before.\n",
    "\n",
    "```python\n",
    "# Evaluate the classifier\n",
    "accuracy = nltk.classify.accuracy(classifier, testing_set) * 100\n",
    "print(f\"\\nClassifier Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Show the most informative features\n",
    "# These are the words the classifier found most indicative of a 'pos' or 'neg' review\n",
    "print(\"\\n--- Most Informative Features ---\")\n",
    "classifier.show_most_informative_features(20)\n",
    "```\n",
    "\n",
    "You will likely see an accuracy between 75% and 85%. The \"most informative features\" output shows which words strongly predict a positive or negative review. For example, `outstanding = True` might have a high `pos:neg` ratio.\n",
    "\n",
    "-----\n",
    "\n",
    "### Step 6: Using Your Trained Model to Classify New Text\n",
    "\n",
    "Finally, let's use our new classifier to predict the sentiment of any new sentence. We must remember to apply the **exact same preprocessing** (tokenizing, lowercasing, etc.) and feature extraction as we did for our training data.\n",
    "\n",
    "```python\n",
    "def classify_sentiment(text):\n",
    "    \"\"\"\n",
    "    Classifies a new piece of text.\n",
    "    \"\"\"\n",
    "    # 1. Tokenize the text\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # 2. Clean the words (lowercase, remove stopwords/punctuation)\n",
    "    clean_words = []\n",
    "    for w in words:\n",
    "        if w.lower() not in stop_words and w.lower() not in string.punctuation:\n",
    "            clean_words.append(w.lower())\n",
    "    \n",
    "    # 3. Extract features using the same function as before\n",
    "    features = find_features(clean_words)\n",
    "    \n",
    "    # 4. Classify\n",
    "    return classifier.classify(features)\n",
    "\n",
    "# --- Test with new sentences ---\n",
    "test_sentence_1 = \"This was an amazing movie! The acting was superb and the plot was thrilling.\"\n",
    "print(f\"'{test_sentence_1}' -> {classify_sentiment(test_sentence_1)}\")\n",
    "\n",
    "test_sentence_2 = \"I was really bored. The entire film felt slow and predictable.\"\n",
    "print(f\"'{test_sentence_2}' -> {classify_sentiment(test_sentence_2)}\")\n",
    "\n",
    "test_sentence_3 = \"The movie was okay, not great but not terrible.\"\n",
    "print(f\"'{test_sentence_3}' -> {classify_sentiment(test_sentence_3)}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
