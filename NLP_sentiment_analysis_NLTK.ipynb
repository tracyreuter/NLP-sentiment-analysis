{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a1ed38",
   "metadata": {},
   "source": [
    "# NLP Sentiment Analysis\n",
    "\n",
    "Perform sentiment analysis using Python's NLTK (Natural Language Toolkit) library.\n",
    "\n",
    "Use the `movie_reviews` corpus, which contains 2,000 movie reviews pre-labeled as either \"positive\" or \"negative.\"\n",
    "\n",
    "Build a **Naive Bayes classifier** - a common and effective model for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04bb8a6",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b887e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 2000 documents with words, bigrams, and trigrams.\n",
      "Example processed document (first 20 tokens):\n",
      "['plot', 'two', 'teen', 'couple', 'go', 'church', 'party', 'drink', 'drive', 'get', 'accident', 'one', 'guy', 'dy', 'but', 'girlfriend', 'continues', 'see', 'life', 'nightmare'] neg\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams, trigrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# # Download data and tokenizer, if needed\n",
    "# nltk.download('movie_reviews')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Load the movie_reviews corpus\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# Create 'documents' as a list of (review, sentiment) tuples\n",
    "documents = []\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        # Add a tuple of (list_of_words, category)\n",
    "        documents.append((list(movie_reviews.words(fileid)), category))\n",
    "\n",
    "# Process documents to include words, bigrams, and trigrams\n",
    "# Bigrams (2-word phrases) and trigrams (3-word phrases) capture context\n",
    "# Example: \"not good\" as bigram, \"not very good\" as trigram\n",
    "processed_documents = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Keep sentiment-critical stop words (e.g. negation), removing them from stop_words set\n",
    "sentiment_words = {\n",
    "    'not', 'no', 'nor', 'never', 'neither', 'none', \n",
    "    'nobody', 'nothing','but', 'however', 'yet', 'although',\n",
    "    'though', 'very', 'too', 'so', 'such', 'quite'\n",
    "}\n",
    "stop_words = stop_words - sentiment_words\n",
    "\n",
    "# Initialize lemmatizer to reduce words to their base form\n",
    "# Lemmatization groups related words: \"amazing\", \"amazingly\", \"amazed\" â†’ \"amaze\"\n",
    "# This reduces feature sparsity and improves generalization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Loop through each document to process words, bigrams, and trigrams\n",
    "for w_list, category in documents:\n",
    "    # Convert to lowercase, lemmatize, remove stopwords and punctuation\n",
    "    # Lemmatization happens AFTER lowercasing for consistency\n",
    "    clean_words = [lemmatizer.lemmatize(w.lower()) for w in w_list \n",
    "                   if w.lower() not in stop_words \n",
    "                   and w.lower() not in string.punctuation]\n",
    "    \n",
    "    # Create bigrams (2-word combinations) to capture phrases like \"not_good\"\n",
    "    # Bigrams help the model understand that \"not good\" has negative sentiment\n",
    "    doc_bigrams = [f\"{w1}_{w2}\" for w1, w2 in bigrams(clean_words)]\n",
    "    \n",
    "    # Create trigrams (3-word combinations) to capture longer phrases like \"not_very_good\"\n",
    "    # Trigrams provide even more context than bigrams for nuanced sentiment\n",
    "    doc_trigrams = [f\"{w1}_{w2}_{w3}\" for w1, w2, w3 in trigrams(clean_words)]\n",
    "    \n",
    "    # Combine individual words, bigrams, and trigrams into a single feature list\n",
    "    processed_documents.append((clean_words + doc_bigrams + doc_trigrams, category))\n",
    "\n",
    "# Confirm documents were processed successfully\n",
    "print(f\"Successfully processed {len(processed_documents)} documents with words, bigrams, and trigrams.\")\n",
    "\n",
    "# Display an example processed document\n",
    "print(\"Example processed document (first 20 tokens):\")\n",
    "print(processed_documents[0][0][:20], processed_documents[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37422a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique features: 1254495\n",
      "Selected features for classification: 27007\n",
      "20 most common features: [('film', 11053), ('but', 8634), ('movie', 6977), ('one', 6028), ('not', 5577), ('character', 3879), ('like', 3789), ('so', 3683), ('time', 2979), ('get', 2814), ('scene', 2671), ('make', 2634), ('even', 2568), ('no', 2472), ('good', 2429), ('story', 2345), ('would', 2109), ('much', 2049), ('also', 1967), ('well', 1921)]\n"
     ]
    }
   ],
   "source": [
    "# Build a features list from processed_documents\n",
    "all_features = []\n",
    "for doc, category in processed_documents:\n",
    "    all_features.extend(doc)\n",
    "\n",
    "# Get the frequency distribution of all features\n",
    "all_features_freq = nltk.FreqDist(all_features)\n",
    "\n",
    "# Select the most frequent features to use for training (dimensionality reduction)\n",
    "word_features = [word for word, count in all_features_freq.items() if count >= 5]\n",
    "\n",
    "# Check results\n",
    "print(f\"Total unique features: {len(all_features_freq)}\")\n",
    "print(f\"Selected features for classification: {len(word_features)}\")\n",
    "print(\"20 most common features:\", all_features_freq.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "429613a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2000 feature sets for training and testing.\n",
      "Example feature set (first 10 features):\n",
      "{'plot': True, 'two': True, 'teen': True, 'couple': True, 'go': True, 'church': True, 'party': True, 'drink': True, 'drive': True, 'get': True} neg\n"
     ]
    }
   ],
   "source": [
    "# Create a feature dictionary\n",
    "def find_features(document_words):\n",
    "    \"\"\"\n",
    "    Takes a list of words from a review and returns a dictionary\n",
    "    of features indicating which of the top words are present.\n",
    "    \"\"\"\n",
    "    # Convert document_words to a set for faster performance\n",
    "    words_in_doc = set(document_words)\n",
    "\n",
    "    # Initialize features dictionary\n",
    "    features = {}\n",
    "\n",
    "    # Loop through all the selected features (words and bigrams)\n",
    "    for w in word_features:\n",
    "        # If the word/bigram is in the document, set True, else False\n",
    "        features[w] = (w in words_in_doc)\n",
    "    \n",
    "    # Return the complete features dictionary\n",
    "    return features\n",
    "\n",
    "# Call the find_features function to create feature sets for processed_documents\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in processed_documents]\n",
    "\n",
    "# Confirm feature sets were created successfully\n",
    "print(f\"Created {len(featuresets)} feature sets for training and testing.\")\n",
    "\n",
    "# Display an example feature set\n",
    "print(\"Example feature set (first 10 features):\")\n",
    "example_features, example_category = featuresets[0]\n",
    "print({k: example_features[k] for k in list(example_features)[:10]}, example_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dafdbe4",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74ab3f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test = train_test_split(featuresets, test_size=0.20, random_state=113)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "clf = nltk.NaiveBayesClassifier.train(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a257be8",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fcbb1295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.0%\n",
      "Precision: 85.8%\n",
      "Recall: 78.9%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy, precision, and recall using sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "y_test = [label for (features, label) in data_test]\n",
    "y_pred = [clf.classify(features) for (features, label) in data_test]\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.1f}%\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred, pos_label='pos') * 100:.1f}%\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred, pos_label='pos') * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a29c6513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  symbol = True              pos : neg    =     14.3 : 1.0\n",
      "              video_game = True              neg : pos    =     13.7 : 1.0\n",
      "                 idiotic = True              neg : pos    =     11.8 : 1.0\n",
      "                  regard = True              pos : neg    =     11.0 : 1.0\n",
      "              astounding = True              pos : neg    =     10.3 : 1.0\n",
      "                  avoids = True              pos : neg    =     10.3 : 1.0\n",
      "              fairy_tale = True              pos : neg    =     10.3 : 1.0\n",
      "             fascination = True              pos : neg    =     10.3 : 1.0\n",
      "                  hatred = True              pos : neg    =     10.3 : 1.0\n",
      "             outstanding = True              pos : neg    =     10.3 : 1.0\n",
      "               strongest = True              pos : neg    =     10.3 : 1.0\n",
      "               stupidity = True              neg : pos    =     10.2 : 1.0\n",
      "               atrocious = True              neg : pos    =      9.7 : 1.0\n",
      "              degenerate = True              neg : pos    =      9.7 : 1.0\n",
      "                  hudson = True              neg : pos    =      9.7 : 1.0\n",
      "               dark_side = True              pos : neg    =      9.6 : 1.0\n",
      "                gripping = True              pos : neg    =      9.6 : 1.0\n",
      "           one_best_film = True              pos : neg    =      9.6 : 1.0\n",
      "              right_time = True              pos : neg    =      9.6 : 1.0\n",
      "               story_one = True              pos : neg    =      9.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Show the most informative features - words most indicative of a positive/negative label\n",
    "clf.show_most_informative_features(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
